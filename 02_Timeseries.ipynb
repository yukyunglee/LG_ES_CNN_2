{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LG 에너지 솔루션]\n",
    "## DX Expert 양성과정 WEEK 2\n",
    "## Convolutional Neural Network 2 - (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 강의 복습\n",
    "Convolutional Neural Network - Part 2\n",
    "\n",
    "* ADENDA 04 CNN for Time-Series Data\n",
    "## 실습 요약\n",
    "1. 본 실습에서는 1D CNN 모델을 활용하여 시계열 분류 문제를 풀이합니다.\n",
    "2. 학습된 모델을 활용하여 평가를 진행합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 0. 환경 구축하기\n",
    "* 필요한 Library 들을 import 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.6.0].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Check torch version & device\n",
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed \n",
    "\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1. 데이터 준비하기\n",
    "금일 실습에서는 Human Activity Recognition Data 데이터를 활용합니다.\n",
    "\n",
    "데이터 설명\n",
    "\n",
    "* Human Activity Recognition (HAR) Data는 30명의 실험자들이 각자 스마트폰을 허리에 착용하고 6가지 활동 (Walking, Walking Upstairs, Walking Downstairs, Sitting, Standing, Laying)을 수행할 때 측정된 센서 데이터로 구성된 데이터셋입니다. 해당 데이터셋은 총 561개의 변수로 이루어져 있으며, 전체 데이터 중 70%는 train 데이터이고 나머지 30%는 test 데이터로 구성됩니다. HAR Data를 활용한 시계열 분류 task는 다변량 시계열 데이터를 input으로 받아 이를 다음 6가지 활동 중 하나의 class로 분류하는 것을 목표로 합니다: 0(Walking), 1(Walking Upstairs), 2(Walking Downstairs), 3(Sitting), 4(Standing), 5(Laying).\n",
    "\n",
    "변수 설명\n",
    "\n",
    "* 독립변수(X): 여러 실험자에 대하여 561개의 변수를 281 시점동안 수집한 시계열 데이터 -> shape: (#실험자, 561, 281)\n",
    "* 종속변수(Y): 시계열 데이터의 label - 0(Walking) / 1(Walking Upstairs) / 2(Walking Downstairs) / 3(Sitting) / 4(Standing) / 5(Laying)\n",
    "\n",
    "데이터 출처\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# github에서 데이터 불러오기\n",
    "!git clone https://github.com/yukyunglee/LG_ES_CNN_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip ./LG_ES_CNN_2/data/har-data.zip -d ./LG_ES_CNN_2/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './LG_ES_CNN_2/data/har-data'\n",
    "BATCH = 32\n",
    "N_CLASS = 6\n",
    "EPOCHS = 100\n",
    "WINDOW_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pickle.load(open(os.path.join(DATA_DIR, 'x_train.pkl'), 'rb'))\n",
    "y = pickle.load(open(os.path.join(DATA_DIR, 'state_train.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 561, 281)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 281)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(data_dir):\n",
    "    # data_dir에 있는 train/test 데이터 불러오기\n",
    "    x = pickle.load(open(os.path.join(data_dir, 'x_train.pkl'), 'rb'))\n",
    "    y = pickle.load(open(os.path.join(data_dir, 'state_train.pkl'), 'rb'))\n",
    "    x_test = pickle.load(open(os.path.join(data_dir, 'x_test.pkl'), 'rb'))\n",
    "    y_test = pickle.load(open(os.path.join(data_dir, 'state_test.pkl'), 'rb'))\n",
    "\n",
    "    # train data를 시간순으로 8:2의 비율로 train/validation set으로 분할\n",
    "    # train, validation, test data의 개수 설정\n",
    "    n_train = int(0.8 * len(x))\n",
    "    n_valid = len(x) - n_train\n",
    "    n_test = len(x_test)\n",
    "    # train/validation set의 개수에 맞게 데이터 분할\n",
    "    x_train, y_train = x[:n_train], y[:n_train]\n",
    "    x_valid, y_valid = x[n_train:], y[n_train:]\n",
    "\n",
    "    return x_train, y_train, n_train, x_valid, y_valid, n_valid, x_test, y_test, n_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3. Pytorch Dataset 정의하기\n",
    "* 딥러닝 프레임워크를 활용하여 학습하기위해서는 물리적인 데이터셋을 학습 가능한 형태로 바꾸어주어야 합니다\n",
    "* 01번 실습자료와 달리 시계열 데이터는 데이터의 형태에 맞게 가공해주는 과정이 중요합니다\n",
    "* 다변량 시계열 데이터를 어떻게 다루어 학습 가능한 형태로 만들어주는지에 집중하면 좋습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_output,window_size):\n",
    "    x_train, y_train, n_train, x_valid, y_valid, n_valid, x_test, y_test, n_test = data_output\n",
    "\n",
    "    # train/validation/test 데이터를 window_size 시점 길이로 분할\n",
    "    datasets = []\n",
    "    for set in [(x_train, y_train, n_train), (x_valid, y_valid, n_valid), (x_test, y_test, n_test)]:\n",
    "        # 전체 시간 길이 설정\n",
    "        T = set[0].shape[-1]\n",
    "        print(set[0].shape)\n",
    "        # 전체 X 데이터를 window_size 크기의 time window로 분할\n",
    "        windows = np.split(set[0][:, :, :window_size * (T // window_size)], (T // window_size), -1)\n",
    "        windows = np.concatenate(windows, 0)\n",
    "        print(windows.shape)\n",
    "        # 전체 y 데이터를 window_size 크기에 맞게 분할\n",
    "        labels = np.split(set[1][:, :window_size * (T // window_size)], (T // window_size), -1)\n",
    "        labels = np.round(np.mean(np.concatenate(labels, 0), -1))\n",
    "        # 분할된 time window 단위의 X, y 데이터를 tensor 형태로 축적\n",
    "        datasets.append(torch.utils.data.TensorDataset(torch.Tensor(windows), torch.Tensor(labels)))\n",
    "        print(\"=\"*10)\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 561, 281)\n",
      "(80, 561, 50)\n",
      "==========\n",
      "(5, 561, 281)\n",
      "(25, 561, 50)\n",
      "==========\n",
      "(9, 561, 288)\n",
      "(45, 561, 50)\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "data_output = data_preprocess(data_dir=DATA_DIR)\n",
    "datasets = create_dataset(data_output, window_size=WINDOW_SIZE)\n",
    "\n",
    "# train/validation/test DataLoader 구축\n",
    "trainset, validset, testset = datasets[0], datasets[1], datasets[2]\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(validset, batch_size=BATCH, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=BATCH, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 4. 모델 정의 및 학습 준비하기\n",
    "* 시계열 데이터를 위한 1D Convolution 모델을 정의하고 학습을 진행합니다\n",
    "* 사전 학습 모델을 사용하지 않고 모델링을 정의하는 방법에 대해 살펴보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-dimensional convolution layer로 구성된 CNN 모델\n",
    "# 2개의 1-dimensional convolution layer와 1개의 fully-connected layer로 구성되어 있음\n",
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        # 첫 번째 1-dimensional convolution layer 구축\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(561, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(2)\n",
    "        )\n",
    "        # 두 번째 1-dimensional convolution layer 구축\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(2)\n",
    "        )\n",
    "        # fully-connected layer 구축\n",
    "        self.fc = nn.Linear(64 * 11, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_1D(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv1d(561, 64, kernel_size=(3,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  )\n",
      "  (fc): Linear(in_features=704, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 1D CNN 구축\n",
    "model = CNN_1D(num_classes=N_CLASS)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer 구축하기\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# loss function 설정\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, num_epochs, optimizer):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # 각 epoch마다 순서대로 training과 validation을 진행\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 모델을 training mode로 설정\n",
    "            else:\n",
    "                model.eval()   # 모델을 validation mode로 설정\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_total = 0\n",
    "\n",
    "            # training과 validation 단계에 맞는 dataloader에 대하여 학습/검증 진행\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device, dtype=torch.long)\n",
    "\n",
    "                # parameter gradients를 0으로 설정\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # training 단계에서만 gradient 업데이트 수행\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # input을 model에 넣어 output을 도출한 후, loss를 계산함\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # output 중 최댓값의 위치에 해당하는 class로 예측을 수행\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward (optimize): training 단계에서만 수행\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # batch별 loss를 축적함\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                running_total += labels.size(0)\n",
    "\n",
    "            # epoch의 loss 및 accuracy 도출\n",
    "            epoch_loss = running_loss / running_total\n",
    "            epoch_acc = running_corrects.double() / running_total\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # validation 단계에서 validation loss가 감소할 때마다 best model 가중치를 업데이트함\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    # 전체 학습 시간 계산\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # validation loss가 가장 낮았을 때의 best model 가중치를 불러와 best model을 구축함\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # best model 가중치 저장\n",
    "    # torch.save(best_model_wts, '../output/best_model.pt')\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trining 단계에서 사용할 Dataloader dictionary 생성\n",
    "dataloaders_dict = {\n",
    "    'train': train_loader,\n",
    "    'val': valid_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "train Loss: 1.7940 Acc: 0.0750\n",
      "val Loss: 1.7858 Acc: 0.3200\n",
      "\n",
      "Epoch 2/100\n",
      "----------\n",
      "train Loss: 1.7800 Acc: 0.3500\n",
      "val Loss: 1.7635 Acc: 0.5600\n",
      "\n",
      "Epoch 3/100\n",
      "----------\n",
      "train Loss: 1.7534 Acc: 0.4250\n",
      "val Loss: 1.7263 Acc: 0.5200\n",
      "\n",
      "Epoch 4/100\n",
      "----------\n",
      "train Loss: 1.7151 Acc: 0.4125\n",
      "val Loss: 1.6654 Acc: 0.5200\n",
      "\n",
      "Epoch 5/100\n",
      "----------\n",
      "train Loss: 1.6518 Acc: 0.4125\n",
      "val Loss: 1.5782 Acc: 0.5200\n",
      "\n",
      "Epoch 6/100\n",
      "----------\n",
      "train Loss: 1.5608 Acc: 0.4125\n",
      "val Loss: 1.4583 Acc: 0.5200\n",
      "\n",
      "Epoch 7/100\n",
      "----------\n",
      "train Loss: 1.4438 Acc: 0.4125\n",
      "val Loss: 1.3427 Acc: 0.5200\n",
      "\n",
      "Epoch 8/100\n",
      "----------\n",
      "train Loss: 1.3369 Acc: 0.4125\n",
      "val Loss: 1.2940 Acc: 0.5200\n",
      "\n",
      "Epoch 9/100\n",
      "----------\n",
      "train Loss: 1.2888 Acc: 0.4125\n",
      "val Loss: 1.3152 Acc: 0.5200\n",
      "\n",
      "Epoch 10/100\n",
      "----------\n",
      "train Loss: 1.2557 Acc: 0.4125\n",
      "val Loss: 1.3479 Acc: 0.5200\n",
      "\n",
      "Epoch 11/100\n",
      "----------\n",
      "train Loss: 1.2283 Acc: 0.4250\n",
      "val Loss: 1.3776 Acc: 0.5600\n",
      "\n",
      "Epoch 12/100\n",
      "----------\n",
      "train Loss: 1.1899 Acc: 0.5000\n",
      "val Loss: 1.3230 Acc: 0.5600\n",
      "\n",
      "Epoch 13/100\n",
      "----------\n",
      "train Loss: 1.1466 Acc: 0.4375\n",
      "val Loss: 1.2469 Acc: 0.5200\n",
      "\n",
      "Epoch 14/100\n",
      "----------\n",
      "train Loss: 1.0994 Acc: 0.4125\n",
      "val Loss: 1.1904 Acc: 0.5200\n",
      "\n",
      "Epoch 15/100\n",
      "----------\n",
      "train Loss: 1.0708 Acc: 0.5125\n",
      "val Loss: 1.1538 Acc: 0.5600\n",
      "\n",
      "Epoch 16/100\n",
      "----------\n",
      "train Loss: 1.0395 Acc: 0.5500\n",
      "val Loss: 1.1443 Acc: 0.6400\n",
      "\n",
      "Epoch 17/100\n",
      "----------\n",
      "train Loss: 0.9880 Acc: 0.5500\n",
      "val Loss: 1.1240 Acc: 0.6400\n",
      "\n",
      "Epoch 18/100\n",
      "----------\n",
      "train Loss: 0.9396 Acc: 0.5625\n",
      "val Loss: 1.1208 Acc: 0.7200\n",
      "\n",
      "Epoch 19/100\n",
      "----------\n",
      "train Loss: 0.8885 Acc: 0.6375\n",
      "val Loss: 1.1142 Acc: 0.7200\n",
      "\n",
      "Epoch 20/100\n",
      "----------\n",
      "train Loss: 0.8423 Acc: 0.7000\n",
      "val Loss: 1.0830 Acc: 0.7600\n",
      "\n",
      "Epoch 21/100\n",
      "----------\n",
      "train Loss: 0.7919 Acc: 0.7000\n",
      "val Loss: 1.0196 Acc: 0.7600\n",
      "\n",
      "Epoch 22/100\n",
      "----------\n",
      "train Loss: 0.7299 Acc: 0.7000\n",
      "val Loss: 0.9846 Acc: 0.7600\n",
      "\n",
      "Epoch 23/100\n",
      "----------\n",
      "train Loss: 0.6781 Acc: 0.7375\n",
      "val Loss: 0.9625 Acc: 0.7600\n",
      "\n",
      "Epoch 24/100\n",
      "----------\n",
      "train Loss: 0.6355 Acc: 0.7875\n",
      "val Loss: 0.9543 Acc: 0.8400\n",
      "\n",
      "Epoch 25/100\n",
      "----------\n",
      "train Loss: 0.5858 Acc: 0.8375\n",
      "val Loss: 0.9303 Acc: 0.8000\n",
      "\n",
      "Epoch 26/100\n",
      "----------\n",
      "train Loss: 0.5439 Acc: 0.8250\n",
      "val Loss: 0.9098 Acc: 0.8000\n",
      "\n",
      "Epoch 27/100\n",
      "----------\n",
      "train Loss: 0.5134 Acc: 0.8500\n",
      "val Loss: 0.9098 Acc: 0.8400\n",
      "\n",
      "Epoch 28/100\n",
      "----------\n",
      "train Loss: 0.4745 Acc: 0.8750\n",
      "val Loss: 0.9211 Acc: 0.8000\n",
      "\n",
      "Epoch 29/100\n",
      "----------\n",
      "train Loss: 0.4313 Acc: 0.8875\n",
      "val Loss: 0.9074 Acc: 0.8400\n",
      "\n",
      "Epoch 30/100\n",
      "----------\n",
      "train Loss: 0.4214 Acc: 0.9125\n",
      "val Loss: 0.9192 Acc: 0.8800\n",
      "\n",
      "Epoch 31/100\n",
      "----------\n",
      "train Loss: 0.3794 Acc: 0.9125\n",
      "val Loss: 0.9588 Acc: 0.8400\n",
      "\n",
      "Epoch 32/100\n",
      "----------\n",
      "train Loss: 0.3735 Acc: 0.9000\n",
      "val Loss: 0.9579 Acc: 0.8400\n",
      "\n",
      "Epoch 33/100\n",
      "----------\n",
      "train Loss: 0.3514 Acc: 0.9000\n",
      "val Loss: 0.9430 Acc: 0.8800\n",
      "\n",
      "Epoch 34/100\n",
      "----------\n",
      "train Loss: 0.3376 Acc: 0.9125\n",
      "val Loss: 0.9519 Acc: 0.8800\n",
      "\n",
      "Epoch 35/100\n",
      "----------\n",
      "train Loss: 0.3271 Acc: 0.9250\n",
      "val Loss: 0.9633 Acc: 0.8800\n",
      "\n",
      "Epoch 36/100\n",
      "----------\n",
      "train Loss: 0.3189 Acc: 0.9125\n",
      "val Loss: 0.9946 Acc: 0.8400\n",
      "\n",
      "Epoch 37/100\n",
      "----------\n",
      "train Loss: 0.3028 Acc: 0.9000\n",
      "val Loss: 0.9915 Acc: 0.8400\n",
      "\n",
      "Epoch 38/100\n",
      "----------\n",
      "train Loss: 0.3062 Acc: 0.9250\n",
      "val Loss: 1.0034 Acc: 0.8800\n",
      "\n",
      "Epoch 39/100\n",
      "----------\n",
      "train Loss: 0.3044 Acc: 0.8875\n",
      "val Loss: 1.0369 Acc: 0.8400\n",
      "\n",
      "Epoch 40/100\n",
      "----------\n",
      "train Loss: 0.2845 Acc: 0.9125\n",
      "val Loss: 1.0291 Acc: 0.8400\n",
      "\n",
      "Epoch 41/100\n",
      "----------\n",
      "train Loss: 0.2828 Acc: 0.9250\n",
      "val Loss: 1.0483 Acc: 0.8400\n",
      "\n",
      "Epoch 42/100\n",
      "----------\n",
      "train Loss: 0.2870 Acc: 0.9000\n",
      "val Loss: 1.0950 Acc: 0.8400\n",
      "\n",
      "Epoch 43/100\n",
      "----------\n",
      "train Loss: 0.2742 Acc: 0.8750\n",
      "val Loss: 1.0589 Acc: 0.8800\n",
      "\n",
      "Epoch 44/100\n",
      "----------\n",
      "train Loss: 0.3235 Acc: 0.9125\n",
      "val Loss: 1.0776 Acc: 0.8800\n",
      "\n",
      "Epoch 45/100\n",
      "----------\n",
      "train Loss: 0.2908 Acc: 0.9250\n",
      "val Loss: 1.1034 Acc: 0.8400\n",
      "\n",
      "Epoch 46/100\n",
      "----------\n",
      "train Loss: 0.2862 Acc: 0.8750\n",
      "val Loss: 1.1351 Acc: 0.8400\n",
      "\n",
      "Epoch 47/100\n",
      "----------\n",
      "train Loss: 0.2721 Acc: 0.8875\n",
      "val Loss: 1.0978 Acc: 0.8400\n",
      "\n",
      "Epoch 48/100\n",
      "----------\n",
      "train Loss: 0.2590 Acc: 0.9125\n",
      "val Loss: 1.1113 Acc: 0.8400\n",
      "\n",
      "Epoch 49/100\n",
      "----------\n",
      "train Loss: 0.2564 Acc: 0.9125\n",
      "val Loss: 1.1270 Acc: 0.8400\n",
      "\n",
      "Epoch 50/100\n",
      "----------\n",
      "train Loss: 0.2479 Acc: 0.9125\n",
      "val Loss: 1.1283 Acc: 0.8400\n",
      "\n",
      "Epoch 51/100\n",
      "----------\n",
      "train Loss: 0.2494 Acc: 0.9125\n",
      "val Loss: 1.1288 Acc: 0.8400\n",
      "\n",
      "Epoch 52/100\n",
      "----------\n",
      "train Loss: 0.2402 Acc: 0.9125\n",
      "val Loss: 1.1559 Acc: 0.8400\n",
      "\n",
      "Epoch 53/100\n",
      "----------\n",
      "train Loss: 0.2483 Acc: 0.8750\n",
      "val Loss: 1.1500 Acc: 0.8400\n",
      "\n",
      "Epoch 54/100\n",
      "----------\n",
      "train Loss: 0.2415 Acc: 0.9125\n",
      "val Loss: 1.1489 Acc: 0.8400\n",
      "\n",
      "Epoch 55/100\n",
      "----------\n",
      "train Loss: 0.2373 Acc: 0.9125\n",
      "val Loss: 1.1592 Acc: 0.8400\n",
      "\n",
      "Epoch 56/100\n",
      "----------\n",
      "train Loss: 0.2430 Acc: 0.9125\n",
      "val Loss: 1.1619 Acc: 0.8400\n",
      "\n",
      "Epoch 57/100\n",
      "----------\n",
      "train Loss: 0.2361 Acc: 0.9125\n",
      "val Loss: 1.1755 Acc: 0.8400\n",
      "\n",
      "Epoch 58/100\n",
      "----------\n",
      "train Loss: 0.2468 Acc: 0.8875\n",
      "val Loss: 1.1977 Acc: 0.8400\n",
      "\n",
      "Epoch 59/100\n",
      "----------\n",
      "train Loss: 0.2392 Acc: 0.8750\n",
      "val Loss: 1.1707 Acc: 0.8800\n",
      "\n",
      "Epoch 60/100\n",
      "----------\n",
      "train Loss: 0.2438 Acc: 0.9125\n",
      "val Loss: 1.1908 Acc: 0.8400\n",
      "\n",
      "Epoch 61/100\n",
      "----------\n",
      "train Loss: 0.2490 Acc: 0.9000\n",
      "val Loss: 1.2287 Acc: 0.8400\n",
      "\n",
      "Epoch 62/100\n",
      "----------\n",
      "train Loss: 0.2336 Acc: 0.8875\n",
      "val Loss: 1.1892 Acc: 0.8800\n",
      "\n",
      "Epoch 63/100\n",
      "----------\n",
      "train Loss: 0.2409 Acc: 0.9125\n",
      "val Loss: 1.2009 Acc: 0.8400\n",
      "\n",
      "Epoch 64/100\n",
      "----------\n",
      "train Loss: 0.2267 Acc: 0.9250\n",
      "val Loss: 1.2300 Acc: 0.8400\n",
      "\n",
      "Epoch 65/100\n",
      "----------\n",
      "train Loss: 0.2545 Acc: 0.8750\n",
      "val Loss: 1.2428 Acc: 0.8400\n",
      "\n",
      "Epoch 66/100\n",
      "----------\n",
      "train Loss: 0.2240 Acc: 0.9125\n",
      "val Loss: 1.2040 Acc: 0.8800\n",
      "\n",
      "Epoch 67/100\n",
      "----------\n",
      "train Loss: 0.2512 Acc: 0.9250\n",
      "val Loss: 1.2153 Acc: 0.8400\n",
      "\n",
      "Epoch 68/100\n",
      "----------\n",
      "train Loss: 0.2196 Acc: 0.9000\n",
      "val Loss: 1.2552 Acc: 0.8400\n",
      "\n",
      "Epoch 69/100\n",
      "----------\n",
      "train Loss: 0.2377 Acc: 0.8625\n",
      "val Loss: 1.2311 Acc: 0.8400\n",
      "\n",
      "Epoch 70/100\n",
      "----------\n",
      "train Loss: 0.2209 Acc: 0.9125\n",
      "val Loss: 1.2256 Acc: 0.8400\n",
      "\n",
      "Epoch 71/100\n",
      "----------\n",
      "train Loss: 0.2176 Acc: 0.9125\n",
      "val Loss: 1.2527 Acc: 0.8400\n",
      "\n",
      "Epoch 72/100\n",
      "----------\n",
      "train Loss: 0.2202 Acc: 0.8875\n",
      "val Loss: 1.2326 Acc: 0.8400\n",
      "\n",
      "Epoch 73/100\n",
      "----------\n",
      "train Loss: 0.2138 Acc: 0.9250\n",
      "val Loss: 1.2296 Acc: 0.8800\n",
      "\n",
      "Epoch 74/100\n",
      "----------\n",
      "train Loss: 0.2399 Acc: 0.9125\n",
      "val Loss: 1.2426 Acc: 0.8400\n",
      "\n",
      "Epoch 75/100\n",
      "----------\n",
      "train Loss: 0.2133 Acc: 0.9000\n",
      "val Loss: 1.2919 Acc: 0.8400\n",
      "\n",
      "Epoch 76/100\n",
      "----------\n",
      "train Loss: 0.2228 Acc: 0.8750\n",
      "val Loss: 1.2530 Acc: 0.8400\n",
      "\n",
      "Epoch 77/100\n",
      "----------\n",
      "train Loss: 0.2150 Acc: 0.9125\n",
      "val Loss: 1.2561 Acc: 0.8400\n",
      "\n",
      "Epoch 78/100\n",
      "----------\n",
      "train Loss: 0.2083 Acc: 0.9250\n",
      "val Loss: 1.2889 Acc: 0.8400\n",
      "\n",
      "Epoch 79/100\n",
      "----------\n",
      "train Loss: 0.2162 Acc: 0.8875\n",
      "val Loss: 1.2755 Acc: 0.8400\n",
      "\n",
      "Epoch 80/100\n",
      "----------\n",
      "train Loss: 0.2271 Acc: 0.9000\n",
      "val Loss: 1.2706 Acc: 0.8400\n",
      "\n",
      "Epoch 81/100\n",
      "----------\n",
      "train Loss: 0.2671 Acc: 0.8625\n",
      "val Loss: 1.3192 Acc: 0.8400\n",
      "\n",
      "Epoch 82/100\n",
      "----------\n",
      "train Loss: 0.1901 Acc: 0.9125\n",
      "val Loss: 1.2631 Acc: 0.8800\n",
      "\n",
      "Epoch 83/100\n",
      "----------\n",
      "train Loss: 0.3019 Acc: 0.9250\n",
      "val Loss: 1.2678 Acc: 0.8800\n",
      "\n",
      "Epoch 84/100\n",
      "----------\n",
      "train Loss: 0.2332 Acc: 0.9000\n",
      "val Loss: 1.3487 Acc: 0.8400\n",
      "\n",
      "Epoch 85/100\n",
      "----------\n",
      "train Loss: 0.2617 Acc: 0.8750\n",
      "val Loss: 1.3088 Acc: 0.8400\n",
      "\n",
      "Epoch 86/100\n",
      "----------\n",
      "train Loss: 0.2112 Acc: 0.9125\n",
      "val Loss: 1.2559 Acc: 0.8800\n",
      "\n",
      "Epoch 87/100\n",
      "----------\n",
      "train Loss: 0.2224 Acc: 0.9125\n",
      "val Loss: 1.2618 Acc: 0.8400\n",
      "\n",
      "Epoch 88/100\n",
      "----------\n",
      "train Loss: 0.2011 Acc: 0.9000\n",
      "val Loss: 1.3197 Acc: 0.8400\n",
      "\n",
      "Epoch 89/100\n",
      "----------\n",
      "train Loss: 0.2364 Acc: 0.9000\n",
      "val Loss: 1.2680 Acc: 0.8400\n",
      "\n",
      "Epoch 90/100\n",
      "----------\n",
      "train Loss: 0.2007 Acc: 0.9125\n",
      "val Loss: 1.2583 Acc: 0.8800\n",
      "\n",
      "Epoch 91/100\n",
      "----------\n",
      "train Loss: 0.2377 Acc: 0.9250\n",
      "val Loss: 1.2723 Acc: 0.8400\n",
      "\n",
      "Epoch 92/100\n",
      "----------\n",
      "train Loss: 0.2715 Acc: 0.8625\n",
      "val Loss: 1.3439 Acc: 0.8400\n",
      "\n",
      "Epoch 93/100\n",
      "----------\n",
      "train Loss: 0.1953 Acc: 0.9125\n",
      "val Loss: 1.2614 Acc: 0.8800\n",
      "\n",
      "Epoch 94/100\n",
      "----------\n",
      "train Loss: 0.3051 Acc: 0.9125\n",
      "val Loss: 1.2568 Acc: 0.8800\n",
      "\n",
      "Epoch 95/100\n",
      "----------\n",
      "train Loss: 0.2214 Acc: 0.9375\n",
      "val Loss: 1.3074 Acc: 0.8400\n",
      "\n",
      "Epoch 96/100\n",
      "----------\n",
      "train Loss: 0.2598 Acc: 0.8750\n",
      "val Loss: 1.3082 Acc: 0.8400\n",
      "\n",
      "Epoch 97/100\n",
      "----------\n",
      "train Loss: 0.2032 Acc: 0.8875\n",
      "val Loss: 1.2339 Acc: 0.8800\n",
      "\n",
      "Epoch 98/100\n",
      "----------\n",
      "train Loss: 0.2545 Acc: 0.9125\n",
      "val Loss: 1.2287 Acc: 0.8800\n",
      "\n",
      "Epoch 99/100\n",
      "----------\n",
      "train Loss: 0.1999 Acc: 0.9250\n",
      "val Loss: 1.3005 Acc: 0.8400\n",
      "\n",
      "Epoch 100/100\n",
      "----------\n",
      "train Loss: 0.2428 Acc: 0.8625\n",
      "val Loss: 1.2577 Acc: 0.8400\n",
      "\n",
      "Training complete in 0m 1s\n",
      "Best val Acc: 0.880000\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model, val_acc_history = train_model(model, dataloaders_dict, criterion, EPOCHS, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()   # 모델을 validation mode로 설정\n",
    "    \n",
    "    # test_loader에 대하여 검증 진행 (gradient update 방지)\n",
    "    with torch.no_grad():\n",
    "        corrects = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "\n",
    "            # forward\n",
    "            # input을 model에 넣어 output을 도출\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # output 중 최댓값의 위치에 해당하는 class로 예측을 수행\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # batch별 정답 개수를 축적함\n",
    "            corrects += torch.sum(preds == labels.data)\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # accuracy를 도출함\n",
    "    test_acc = corrects.double() / total\n",
    "    print('Testing Acc: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Acc: 0.8222\n"
     ]
    }
   ],
   "source": [
    "# 모델 검증 (Acc: 0.8222)\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('loganomaly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39a427de25d3b2d4719acd8b5feb1de000c6740ecfc3a029c8a926c48371f27d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
